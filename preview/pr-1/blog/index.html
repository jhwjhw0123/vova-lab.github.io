<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Project | random lab site</title>

<link rel="icon" href="/vova-lab.github.io/preview/pr-/images/icon.png">

<meta name="title" content="Project">
<meta name="description" content="An engaging 1-3 sentence description of your lab.">

<meta property="og:title" content="Project">
<meta property="og:site_title" content="random lab site">
<meta property="og:description" content="An engaging 1-3 sentence description of your lab.">
<meta property="og:url" content="/vova-lab.github.io/preview/pr-">
<meta property="og:image" content="/vova-lab.github.io/preview/pr-/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Project">
<meta property="twitter:description" content="An engaging 1-3 sentence description of your lab.">
<meta property="twitter:url" content="/vova-lab.github.io/preview/pr-">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/vova-lab.github.io/preview/pr-/images/share.jpg">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Project",
    "description": "An engaging 1-3 sentence description of your lab.",
    "headline": "Project",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/vova-lab.github.io/preview/pr-/images/icon.png" }
    },
    "url": "/vova-lab.github.io/preview/pr-"
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/vova-lab.github.io/preview/pr-/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet" media="none" onload="this.removeAttribute('media'); this.onload = null;">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/vova-lab.github.io/preview/pr-/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/all.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/background.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/body.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/button.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/card.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/code.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/details.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/float.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/font.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/form.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/header.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/image.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/link.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/list.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/main.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/section.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/table.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/vova-lab.github.io/preview/pr-/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/vova-lab.github.io/preview/pr-/_scripts/anchors.js"></script>

  <script src="/vova-lab.github.io/preview/pr-/_scripts/dark-mode.js"></script>

  <script src="/vova-lab.github.io/preview/pr-/_scripts/fetch-tags.js"></script>

  <script src="/vova-lab.github.io/preview/pr-/_scripts/search.js"></script>

  <script src="/vova-lab.github.io/preview/pr-/_scripts/site-search.js"></script>

  <script src="/vova-lab.github.io/preview/pr-/_scripts/table-wrap.js"></script>

  <script src="/vova-lab.github.io/preview/pr-/_scripts/tooltip.js"></script>


</head>

  <body>
    







<header class="background" style="--image: url('/vova-lab.github.io/preview/pr-/vova-lab.github.io/preview/pr-/images/background.jpg')" data-dark="true">
  <a href="/vova-lab.github.io/preview/pr-/" class="home">
    
      <span class="logo">
        
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-40 -60 80 100">
  <style>
    .bubble {
      animation: float 2s ease-out both infinite var(--delay);
    }
    @keyframes float {
      0% {
        opacity: 0;
      }
      50% {
        transform: translateY(0);
        opacity: 0;
      }
      75% {
        opacity: 1;
      }
      100% {
        opacity: 0;
        transform: translateY(-40px);
      }
    }
  </style>
  <g fill="currentColor" opacity="0.5">
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 0.1s"></circle>
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 0.4s"></circle>
    <circle class="bubble" cx="0" cy="-10" r="3" style="--delay: 1.1s"></circle>
  </g>
  <path fill="#38bdf8" d="
      M 0 -22.5
      L -19.5 -11.25
      L -19.5 11.25
      L 0 22.5
      L 19.5 11.25
      L 19.5 -11.25
      z
    "></path>
  <path fill="#bae6fd" d="
      M 0 -22.5
      L -19.5 -11.25
      L 0 0
      L 19.5 -11.25
      z
    "></path>
  <path fill="none" stroke="currentColor" stroke-width="5" d="
      M -18 -53
      L -10 -53
      L -10 -29.2
      L -30.3 -17.5
      L -30.3 17.5
      L 0 35
      L 30.3 17.5
      L 30.3 -17.5
      L 10 -29.2
      L 10 -53
      L 18 -53
    "></path>
</svg>

        
      </span>
    
    
      <span class="title-text" data-tooltip="Home">
        
          <span class="title">random lab site</span>
        
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/vova-lab.github.io/preview/pr-/research/" data-tooltip="Published works">
          Research
        </a>
      
    
      
        <a href="/vova-lab.github.io/preview/pr-/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/vova-lab.github.io/preview/pr-/blog/" data-tooltip="Musings and miscellany">
          Project
        </a>
      
    
      
        <a href="/vova-lab.github.io/preview/pr-/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - filter out blank sections
-->






  
  
  

  <section class="background" data-size="page">
    <h1 id="project">
<i class="icon fa-solid fa-feather-pointed"></i>Project</h1>

<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.
Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.</p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<div class="search-box">
  <input type="text" class="search-input" oninput="onSearchInput(this)" placeholder="Search items on this page">
  <button disabled data-tooltip="Clear search" aria-label="clear search" onclick="onSearchClear()">
    <i class="icon fa-solid fa-magnifying-glass"></i>
  </button>
</div>

<div class="tags" data-link="/vova-lab.github.io/preview/pr-/blog/">
    
      <a href="/blog/?search=%22tag:%20personalized-healthcare%22" class="tag" data-tooltip='Show items with the tag "personalized-healthcare"'>
        personalized-healthcare
      </a>
    
      <a href="/blog/?search=%22tag:%20distributed-learning%22" class="tag" data-tooltip='Show items with the tag "distributed-learning"'>
        distributed-learning
      </a>
    
      <a href="/blog/?search=%22tag:%20continual-learning%22" class="tag" data-tooltip='Show items with the tag "continual-learning"'>
        continual-learning
      </a>
    
      <a href="/blog/?search=%22tag:%20medical-imaging%22" class="tag" data-tooltip='Show items with the tag "medical-imaging"'>
        medical-imaging
      </a>
    
      <a href="/blog/?search=%22tag:%20low-compute-optimization%22" class="tag" data-tooltip='Show items with the tag "low-compute-optimization"'>
        low-compute-optimization
      </a>
    
      <a href="/blog/?search=%22tag:%20biomarker%22" class="tag" data-tooltip='Show items with the tag "biomarker"'>
        biomarker
      </a>
    
      <a href="/blog/?search=%22tag:%20few-shot-learning%22" class="tag" data-tooltip='Show items with the tag "few-shot-learning"'>
        few-shot-learning
      </a>
    
      <a href="/blog/?search=%22tag:%20multi-label-classification%22" class="tag" data-tooltip='Show items with the tag "multi-label-classification"'>
        multi-label-classification
      </a>
    
      <a href="/blog/?search=%22tag:%20wearable-device%22" class="tag" data-tooltip='Show items with the tag "wearable-device"'>
        wearable-device
      </a>
    
      <a href="/blog/?search=%22tag:%20cardiovascular-health%22" class="tag" data-tooltip='Show items with the tag "cardiovascular-health"'>
        cardiovascular-health
      </a>
    
      <a href="/blog/?search=%22tag:%20explainability%22" class="tag" data-tooltip='Show items with the tag "explainability"'>
        explainability
      </a>
    
      <a href="/blog/?search=%22tag:%20interpretability%22" class="tag" data-tooltip='Show items with the tag "interpretability"'>
        interpretability
      </a>
    
      <a href="/blog/?search=%22tag:%20diagnositic-imaging%22" class="tag" data-tooltip='Show items with the tag "diagnositic-imaging"'>
        diagnositic-imaging
      </a>
    
      <a href="/blog/?search=%22tag:%20unsupervised-learning%22" class="tag" data-tooltip='Show items with the tag "unsupervised-learning"'>
        unsupervised-learning
      </a>
    
      <a href="/blog/?search=%22tag:%20multiple-sclerosis%22" class="tag" data-tooltip='Show items with the tag "multiple-sclerosis"'>
        multiple-sclerosis
      </a>
    
      <a href="/blog/?search=%22tag:%20brain-mri%22" class="tag" data-tooltip='Show items with the tag "brain-mri"'>
        brain-mri
      </a>
    
      <a href="/blog/?search=%22tag:%20scanning-electron-microscopy%22" class="tag" data-tooltip='Show items with the tag "scanning-electron-microscopy"'>
        scanning-electron-microscopy
      </a>
    
      <a href="/blog/?search=%22tag:%20energy-dispersive-spectroscopy%22" class="tag" data-tooltip='Show items with the tag "energy-dispersive-spectroscopy"'>
        energy-dispersive-spectroscopy
      </a>
    
      <a href="/blog/?search=%22tag:%20convolutional-neural-networks%22" class="tag" data-tooltip='Show items with the tag "convolutional-neural-networks"'>
        convolutional-neural-networks
      </a>
    
      <a href="/blog/?search=%22tag:%20semantic-segmentation%22" class="tag" data-tooltip='Show items with the tag "semantic-segmentation"'>
        semantic-segmentation
      </a>
    
      <a href="/blog/?search=%22tag:%20nuclear-energy%22" class="tag" data-tooltip='Show items with the tag "nuclear-energy"'>
        nuclear-energy
      </a>
    
  </div>

<div class="search-info"></div>

<h3 id="2025">2025</h3>

<div class="post-excerpt-container">
  <div class="post-excerpt">
    
    
    

    

    <div class="post-excerpt-text">
      <a href="/vova-lab.github.io/preview/pr-/2025/03/17/ALP.html">Automatic Active Lesion Tracking in Multiple Sclerosis Using Unsupervised Machine Learning</a>

      <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Jason Uwaeze</span>
      </span>
    
  

  
  

  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>March 17, 2025</span>
    </span>
  

  
</div>


  


  <div class="tags" data-link="/vova-lab.github.io/preview/pr-/blog">
    
      <a href="blog?search=%22tag:%20diagnositic-imaging%22" class="tag" data-tooltip='Show items with the tag "diagnositic-imaging"'>
        diagnositic-imaging
      </a>
    
      <a href="blog?search=%22tag:%20unsupervised-learning%22" class="tag" data-tooltip='Show items with the tag "unsupervised-learning"'>
        unsupervised-learning
      </a>
    
      <a href="blog?search=%22tag:%20multiple-sclerosis%22" class="tag" data-tooltip='Show items with the tag "multiple-sclerosis"'>
        multiple-sclerosis
      </a>
    
      <a href="blog?search=%22tag:%20brain-mri%22" class="tag" data-tooltip='Show items with the tag "brain-mri"'>
        brain-mri
      </a>
    
  </div>





      
      
      <p data-search="p This study uses Non-linear Dimensionality Reduction NLDR techniques to identify active lesions in magnetic resonance imaging MRI . We applied Locally Linear Embedding LLE and Isometric Feature Mapping Isomap to MRI data from 40 multiple sclerosis patients, achieving median Dice scores of 0.74 and 0.78 for active lesion segmentation, respectively a href https: www.mdpi.com 2075-4418 14 6 632 Paper a a href https: github.com Wazhee Automatic-Multiple-Sclerosis-Lesion-Tracking Code a . p figure class figure a class figure-image aria-label The Isomap algorithm can be summarized in four steps: 1 select a nearest neighbor algorithm, 2 calculate geodesic distances for all data points 3 apply multidimensional scaling to geodesic distance matrix, 4 output single embedded image. img src vova-lab.github.io preview pr- images active lesion tracking.png style width: 1000px; max-height: unset; alt The Isomap algorithm can be summarized in four steps: 1 select a nearest neighbor algorithm, 2 calculate geodesic distances for all data points 3 apply multidimensional scaling to geodesic distance matrix, 4 output single embedded image. loading lazy onerror this.src vova-lab.github.io preview pr- images fallback.svg ; this.onerror null; a figcaption class figure-caption The Isomap algorithm can be summarized in four steps: 1 select a nearest neighbor algorithm, 2 calculate geodesic distances for all data points 3 apply multidimensional scaling to geodesic distance matrix, 4 output single embedded image. figcaption figure p The objective of this study was to demonstrate that unsupervised NLDR methods outperform supervised machine learning methods in identifying active lesions. To accomplish this, we introduce key differences between LDR and NLDR. Subsequently, we establish the motivation behind applying NLDR to brain MRI data. Then, we provide details for the MR imaging dataset and performance evaluation metrics used in this work. Furthermore, we compare NLDR methods to current state-of-the-art methods for active MS lesion identification. Lastly, we discuss the potential limitations of this work. Paper published in em Multidisciplinary Digital Publishing Institute em MDPI 2024, a href https: www.mdpi.com 2075-4418 14 6 632 Automatic Active Lesion Tracking in Multiple Sclerosis Using Unsupervised Machine Learning a . p p This project is funded by the Defense Advanced Research Projects Agency a href https: intelligencecommunitynews.com darpa-launches-shell-program DARPA a , Navy, the National GEM consortium a href https: www.gemfellowship.org DARPA a and University of Texas Health Science Center Houston, UT-Health Endowed Chair in Biomedical Engineering for Michael A. Jacobs, PhD. p">
        This study uses Non-linear Dimensionality Reduction (NLDR) techniques to identify active lesions in magnetic resonance imaging (MRI). We applied Locally Linear Embedding (LLE) and Isometric Feature Mapping (Isomap) to MRI data from 40 multiple sclerosis patients, achieving median Dice scores of 0.74 and 0.78 for active lesion segmentation, respectively [Paper][Code].

      </p>
    </div>
  </div>
</div>

<div class="post-excerpt-container">
  <div class="post-excerpt">
    
    
    

    

    <div class="post-excerpt-text">
      <a href="/vova-lab.github.io/preview/pr-/2025/03/17/PBMC.html">Patch-Based Convolutional Neural Networks for Multiple Microstructural Features Detection in Nuclear Fuel</a>

      <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Jason Uwaeze</span>
      </span>
    
  

  
  

  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>March 17, 2025</span>
    </span>
  

  
</div>


  


  <div class="tags" data-link="/vova-lab.github.io/preview/pr-/blog">
    
      <a href="blog?search=%22tag:%20scanning-electron-microscopy%22" class="tag" data-tooltip='Show items with the tag "scanning-electron-microscopy"'>
        scanning-electron-microscopy
      </a>
    
      <a href="blog?search=%22tag:%20energy-dispersive-spectroscopy%22" class="tag" data-tooltip='Show items with the tag "energy-dispersive-spectroscopy"'>
        energy-dispersive-spectroscopy
      </a>
    
      <a href="blog?search=%22tag:%20convolutional-neural-networks%22" class="tag" data-tooltip='Show items with the tag "convolutional-neural-networks"'>
        convolutional-neural-networks
      </a>
    
      <a href="blog?search=%22tag:%20semantic-segmentation%22" class="tag" data-tooltip='Show items with the tag "semantic-segmentation"'>
        semantic-segmentation
      </a>
    
      <a href="blog?search=%22tag:%20nuclear-energy%22" class="tag" data-tooltip='Show items with the tag "nuclear-energy"'>
        nuclear-energy
      </a>
    
  </div>





      
      
      <p data-search="p This work introduces a new framework for identifying microstructures in irradiated U-10Zr wt. metallic fuel with limited annotated data. The framework includes the creation of a reliableannotated dataset with paired SEM and ground truth data from EDS maps, the applications ofCNNs for microstructure identification, and the validation of model performance. We evaluate several models,including Patch-based U-Net, Attention U-Net, and Residual U-Net, finding that patch-based U-Net exhibits superior segmentation performance and consistency. This approach reduces reliance on EDS detectors and aids in accelerating nuclear material analysis process, highlighting thepotential of advanced deep learning techniques to improve microstructural understanding in nuclear material. p p a href https: www.mdpi.com 2075-4418 14 6 632 Paper a a href https: github.com Wazhee Automatic-Multiple-Sclerosis-Lesion-Tracking Code a . p figure class figure a class figure-image aria-label Framework for patch-based material characterization img src vova-lab.github.io preview pr- images pbmc.png style width: 1000px; max-height: unset; alt Framework for patch-based material characterization loading lazy onerror this.src vova-lab.github.io preview pr- images fallback.svg ; this.onerror null; a figcaption class figure-caption Framework for patch-based material characterization figcaption figure p Focused ion beam scanning electron microscopy FIB-SEM tomography has increasingly beenutilized for acquiring three-dimensional 3D microstructure features at the sub-micron scale inirradiated nuclear materials. Despite its growing use, several challenges persist. These include the time-intensive nature ofdata collection of EDS data, difficulties in distinguishing between various microstructures, and issues with image alignment. These challenges currently limit the broader application of FIB-SEM tomography in the field. To overcome these limitations, we propose using convolutionalneural networks CNNs to automate microstructure identification in SEM images. Our study introduces a new framework for identifying microstructures in irradiated U-10Zr wt. metallic fuel with limited annotated data. The framework includes the creation of a reliable annotated dataset with paired SEM and ground truth data from EDS maps, the applications of CNNs for microstructure identification, and the validation of model performance. Specifically,we employed the Segment Anything Model SAM to align SEM images with corresponding EDS maps and focused ion beam FIB tomography SEM data. We evaluate several models, including Patch-based U-Net, Attention U-Net, and Residual U-Net, finding that patch-based U- Net exhibits superior segmentation performance and consistency. p p The key contributions of this work include: 1 Developing a new framework for data preparation in FIB-SEM tomography data. 2 Implementing PBCNNs for efficient segmentation of material microstructures in SEMimages. 3 Performed a comprehensive analysis of state-of-the-art CNN-based SEM segmentationmethod p p Paper submitted to em Scientific Reports em Nature 2025, a href https: www.nature.com srep Patch-Based Convolutional Neural Networks for Multiple Microstructural Features Detection in FIB-SEM Micrographs of Irradiated Nuclear Fuel a . p p This work was supported by U.S. Department of Energy DOE , INL Laboratory Directed Research amp; Development LDRD Program under DOE Idaho Operations Office Contract DE-AC07-05ID14517 tracking number:23A1070-069FP, 24A1081-149FP, 25A1090-192FP . This research also made use of the resources ofthe High Performance Computing HPC Center at Idaho National Laboratory, which issupported by the Office of Nuclear Energy of the U.S. Department of Energy and the NuclearScience User Facilities. p">
        This work introduces a new framework for identifying microstructures in irradiated U-10Zr (wt. %)
metallic fuel with limited annotated data. The framework includes the creation of a reliable
annotated dataset with paired SEM and ground truth data from EDS maps, the applications of
CNNs for microstructure identification, and the validation of model performance. We evaluate several models,
including Patch-based U-Net, Attention U-Net, and Residual U-Net, finding that patch-based U-Net exhibits superior segmentation performance and consistency. This approach reduces reliance on EDS detectors and aids in accelerating nuclear material analysis process, highlighting the
potential of advanced deep learning techniques to improve microstructural understanding in nuclear material.

      </p>
    </div>
  </div>
</div>

<h3 id="2024">2024</h3>

<div class="post-excerpt-container">
  <div class="post-excerpt">
    
    
    

    

    <div class="post-excerpt-text">
      <a href="/vova-lab.github.io/preview/pr-/2024/12/30/ADFLL.html">Asynchronous Decentralized Federated Lifelong Learning (ADFLL)</a>

      <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Guangyao Zheng</span>
      </span>
    
  

  
  

  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>December 30, 2024</span>
    </span>
  

  
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span>March 17, 2025</span>
    </span>
  
</div>


  


  <div class="tags" data-link="/vova-lab.github.io/preview/pr-/blog">
    
      <a href="blog?search=%22tag:%20personalized-healthcare%22" class="tag" data-tooltip='Show items with the tag "personalized-healthcare"'>
        personalized-healthcare
      </a>
    
      <a href="blog?search=%22tag:%20distributed-learning%22" class="tag" data-tooltip='Show items with the tag "distributed-learning"'>
        distributed-learning
      </a>
    
      <a href="blog?search=%22tag:%20continual-learning%22" class="tag" data-tooltip='Show items with the tag "continual-learning"'>
        continual-learning
      </a>
    
      <a href="blog?search=%22tag:%20medical-imaging%22" class="tag" data-tooltip='Show items with the tag "medical-imaging"'>
        medical-imaging
      </a>
    
      <a href="blog?search=%22tag:%20low-compute-optimization%22" class="tag" data-tooltip='Show items with the tag "low-compute-optimization"'>
        low-compute-optimization
      </a>
    
  </div>





      
      
      <p data-search="p This project introduces the Asynchronous Decentralized Federated Lifelong Learning ADFLL framework, an innovative approach to federated learning that addresses the limitations of synchronous training schedules and the lack of lifelong learning in conventional machine learning frameworks for medical applications. ADFLL enables asynchronous and continual learning across agents, allowing them to leverage both their own experiences and knowledge shared by others. The framework was evaluated using deep reinforcement learning DRL for landmark localization tasks across diverse imaging modalities, orientations, and sequences. Experimental results demonstrated that ADFLL outperforms baseline models in collaborative learning, showing superior performance on both in-distribution and out-of-distribution test sets. This robust, efficient, and flexible framework is well-suited for deployment in real-world applications requiring privacy-preserving and lifelong collaborative learning. Paper published in em Medical Imaging with Deep Learning em MIDL 2024, a href https: openreview.net forum id FbM7sDDAZ4 Towards a Collective Medical Imaging AI: Enabling Continual Learning from Peers a . Github repo: https: github.com guangyaoz ADFLL. p figure class figure a class figure-image aria-label Illustration of asynchronous decentralized federated lifelong learning ADFLL set up for cross-modality 3D localization of spleen. The blue, orange, and green boxes represent different agents in the setup. Each agent sequentially encounters two different imaging modalities for along with experiences shared by the other nodes, enabling them to learn to localize the spleen across all four modalities as opposed to just the two they encountered . img src vova-lab.github.io preview pr- images ADFLL Concept Figure.png style width: 1000px; max-height: unset; alt Illustration of asynchronous decentralized federated lifelong learning ADFLL set up for cross-modality 3D localization of spleen. The blue, orange, and green boxes represent different agents in the setup. Each agent sequentially encounters two different imaging modalities for along with experiences shared by the other nodes, enabling them to learn to localize the spleen across all four modalities as opposed to just the two they encountered . loading lazy onerror this.src vova-lab.github.io preview pr- images fallback.svg ; this.onerror null; a figcaption class figure-caption Illustration of asynchronous decentralized federated lifelong learning ADFLL set up for cross-modality 3D localization of spleen. The blue, orange, and green boxes represent different agents in the setup. Each agent sequentially encounters two different imaging modalities for along with experiences shared by the other nodes, enabling them to learn to localize the spleen across all four modalities as opposed to just the two they encountered . figcaption figure p To further enhance the efficiency of the ADFLL framework, we integrate a reward distribution-preserving coreset compression technique for selective experience replay buffers. This method compresses stored experiences, reducing computational overhead while maintaining the model s ability to mitigate catastrophic forgetting. Evaluated on tasks such as ventricle localization in the BRATS dataset and landmark localization in whole-body MRI, the compressed lifelong learning models demonstrated competitive performance with minimal impact on accuracy. For instance, the 10x compressed models achieved mean pixel distances close to conventional lifelong learning models, highlighting the viability of coreset compression in resource-constrained settings. This addition reinforces ADFLL s scalability and adaptability to real-world applications. Paper published in em Medical Imaging with Deep Learning em MIDL 2023, a href https: proceedings.mlr.press v227 zheng24a.html Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging a . p p To further address the challenges of deploying the ADFLL framework on low-compute edge devices in rapidly evolving imaging environments, we developed three image coreset algorithms for compressing and denoising medical images in selective experience replay. These include neighborhood averaging, neighborhood sensitivity-based sampling, and maximum entropy coresets, which achieve 27x compression while maintaining strong performance in localizing anatomical landmarks on DIXON water and fat MRI images. Notably, the maximum entropy coreset outperformed conventional lifelong learning models with an average distance error of 11.97 12.02 compared to 19.24 50.77, showcasing its potential to enhance efficiency and adaptability in real-world medical imaging applications. Paper on arXiv: a href https: arxiv.org abs 2306.05310 A framework for dynamically training and adapting deep reinforcement learning models to different, low-compute, and continuously changing radiology deployment environments a . p p This project is funded by DARPA, part of the Shared Experience Lifelong Learning a href https: intelligencecommunitynews.com darpa-launches-shell-program ShELL a program. Collectively, we have published a paper on em Nature Machine Intelligence em , a href https: rdcu.be dB9zt A collective AI via lifelong learning and sharing at the edge a . p">
        This project introduces the Asynchronous Decentralized Federated Lifelong Learning (ADFLL) framework, an innovative approach to federated learning that addresses the limitations of synchronous training schedules and the lack of lifelong learning in conventional machine learning frameworks for medical applications. ADFLL enables asynchronous and continual learning across agents, allowing them to leverage both their own experiences and knowledge shared by others. The framework was evaluated using deep reinforcement learning (DRL) for landmark localization tasks across diverse imaging modalities, orientations, and sequences. Experimental results demonstrated that ADFLL outperforms baseline models in collaborative learning, showing superior performance on both in-distribution and out-of-distribution test sets. This robust, efficient, and flexible framework is well-suited for deployment in real-world applications requiring privacy-preserving and lifelong collaborative learning. Paper published in Medical Imaging with Deep Learning (MIDL) 2024, Towards a Collective Medical Imaging AI: Enabling Continual Learning from Peers. Github repo: https://github.com/guangyaoz/ADFLL.

      </p>
    </div>
  </div>
</div>

<div class="post-excerpt-container">
  <div class="post-excerpt">
    
    
    

    

    <div class="post-excerpt-text">
      <a href="/vova-lab.github.io/preview/pr-/2024/12/30/Biomarker.html">Biomarker Stroke Prediction</a>

      <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Guangyao Zheng</span>
      </span>
    
  

  
  

  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>December 30, 2024</span>
    </span>
  

  
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span>March 17, 2025</span>
    </span>
  
</div>


  


  <div class="tags" data-link="/vova-lab.github.io/preview/pr-/blog">
    
      <a href="blog?search=%22tag:%20personalized-healthcare%22" class="tag" data-tooltip='Show items with the tag "personalized-healthcare"'>
        personalized-healthcare
      </a>
    
      <a href="blog?search=%22tag:%20biomarker%22" class="tag" data-tooltip='Show items with the tag "biomarker"'>
        biomarker
      </a>
    
      <a href="blog?search=%22tag:%20few-shot-learning%22" class="tag" data-tooltip='Show items with the tag "few-shot-learning"'>
        few-shot-learning
      </a>
    
  </div>





      
      
      <p data-search="p This project explores the link between mitochondrial oxidative phosphorylation OxPhos abnormalities and stroke risk in patients with advanced congestive heart failure CHF undergoing continuous-flow left ventricular assist device CF-LVAD implantation. Stroke remains a significant complication for this patient population, and prior ischemic events may predispose individuals to systemic mitochondrial dysfunction, exacerbating their risk of new strokes post-implantation. p p In this study, OxPhos complex proteins complex I C.I through complex V C.V were measured in blood leukocytes of 50 CF-LVAD patients, evenly split between those with and without prior stroke histories. Key findings revealed: p p Patients with a history of stroke exhibited significantly lower levels of C.I, C.II, C.IV, and C.V proteins in both pre-and post-CF-LVAD implantation compared to those without prior strokes.Post-CF-LVAD, oxidative phosphorylation protein levels were markedly reduced in the prior-stroke group compared to baseline.Using machine learning techniques, including Least Absolute Shrinkage and Selection Operator LASSO and Random Forest models, the study identified six prognostic factors that accurately predicted postoperative stroke risk, achieving an area under the receiver operating characteristic ROC curve AUC of 0.93. These findings highlight a novel association between mitochondrial dysfunction at the systemic level and stroke risk in this patient group. p p The project underscores the potential of OxPhos protein expression as a biomarker for identifying patients at heightened risk of stroke following CF-LVAD implantation. Further research will refine these biomarkers and explore targeted interventions to mitigate postoperative stroke risk in CHF patients. p p Paper published in em American Society for Artificial Internal Organs em ASAIO : a href https: journals.lww.com asaiojournal abstract 9900 machine learning assisted stroke prediction in.586.aspx Machine Learning Assisted Stroke Prediction in Mechanical Circulatory Support: Predictive Role of Systemic Mitochondrial Dysfunction a p">
        This project explores the link between mitochondrial oxidative phosphorylation (OxPhos) abnormalities and stroke risk in patients with advanced congestive heart failure (CHF) undergoing continuous-flow left ventricular assist device (CF-LVAD) implantation. Stroke remains a significant complication for this patient population, and prior ischemic events may predispose individuals to systemic mitochondrial dysfunction, exacerbating their risk of new strokes post-implantation.

      </p>
    </div>
  </div>
</div>

<div class="post-excerpt-container">
  <div class="post-excerpt">
    
    
    

    

    <div class="post-excerpt-text">
      <a href="/vova-lab.github.io/preview/pr-/2024/12/30/arrhythmia.html">Arrhythmia Detection and ECG Explainability</a>

      <div class="post-info">
  
    
    
      <span data-tooltip="Author">
        <i class="icon fa-solid fa-feather-pointed"></i>
        <span>Guangyao Zheng</span>
      </span>
    
  

  
  

  
    <span data-tooltip="Originally published on">
      <i class="icon fa-regular fa-calendar"></i>
      <span>December 30, 2024</span>
    </span>
  

  
    <span data-tooltip="Last updated on">
      <i class="icon fa-solid fa-clock-rotate-left"></i>
      <span>March 17, 2025</span>
    </span>
  
</div>


  


  <div class="tags" data-link="/vova-lab.github.io/preview/pr-/blog">
    
      <a href="blog?search=%22tag:%20personalized-healthcare%22" class="tag" data-tooltip='Show items with the tag "personalized-healthcare"'>
        personalized-healthcare
      </a>
    
      <a href="blog?search=%22tag:%20multi-label-classification%22" class="tag" data-tooltip='Show items with the tag "multi-label-classification"'>
        multi-label-classification
      </a>
    
      <a href="blog?search=%22tag:%20wearable-device%22" class="tag" data-tooltip='Show items with the tag "wearable-device"'>
        wearable-device
      </a>
    
      <a href="blog?search=%22tag:%20cardiovascular-health%22" class="tag" data-tooltip='Show items with the tag "cardiovascular-health"'>
        cardiovascular-health
      </a>
    
      <a href="blog?search=%22tag:%20explainability%22" class="tag" data-tooltip='Show items with the tag "explainability"'>
        explainability
      </a>
    
      <a href="blog?search=%22tag:%20interpretability%22" class="tag" data-tooltip='Show items with the tag "interpretability"'>
        interpretability
      </a>
    
  </div>





      
      
      <p data-search="p This project addresses the critical challenges of arrhythmia detection and classification, particularly in the context of wearable electrocardiogram ECG monitoring devices. Unlike clinically controlled environments, wearable devices operate in noisy, real-world conditions, which complicates the accurate identification of arrhythmias. Additionally, the inherent imbalance in the ratio of normal heartbeats to arrhythmic ones, along with the diverse combinations of arrhythmia types, further compounds the difficulty of the task. p p To tackle these challenges, we developed a novel hierarchical deep learning model that combines Convolutional Neural Networks CNN , Bidirectional Long Short-Term Memory networks BiLSTM , and an Attention mechanism. Our framework consists of two key modules: p ol li A binary classification module to distinguish normal heartbeats from arrhythmic ones. li li A multi-label classification module to categorize arrhythmia events across combinations of beat and rhythm types.The model was trained and evaluated on a proprietary dataset, achieving state-of-the-art performance metrics: li ol figure class figure a class figure-image aria-label A to D is a flowchart of the proposed framework. C to D are proposed concepts of a hierarchical approach. A Multi-labeled wireless ECG arrhythmia raw data, B 4-beat input data after preprocessing, C Binary classification model for normal heartbeat and arrhythmia classification, D Multi-class, multi-label arrhythmia classification model, E Detailed structure of the proposed CNN BiLSTM with attention model. img src vova-lab.github.io preview pr- images arrhythmia.png style width: 1000px; max-height: unset; alt A to D is a flowchart of the proposed framework. C to D are proposed concepts of a hierarchical approach. A Multi-labeled wireless ECG arrhythmia raw data, B 4-beat input data after preprocessing, C Binary classification model for normal heartbeat and arrhythmia classification, D Multi-class, multi-label arrhythmia classification model, E Detailed structure of the proposed CNN BiLSTM with attention model. loading lazy onerror this.src vova-lab.github.io preview pr- images fallback.svg ; this.onerror null; a figcaption class figure-caption A to D is a flowchart of the proposed framework. C to D are proposed concepts of a hierarchical approach. A Multi-labeled wireless ECG arrhythmia raw data, B 4-beat input data after preprocessing, C Binary classification model for normal heartbeat and arrhythmia classification, D Multi-class, multi-label arrhythmia classification model, E Detailed structure of the proposed CNN BiLSTM with attention model. figcaption figure p Binary Classification: Accuracy: 95 , F1-score: 0.838, AUC: 0.906. Multi-label Classification: Accuracy: 88 , F1-score: 0.736, AUC: 0.875.We benchmarked our framework against strong baselines, including CNN BiGRU with Attention, ConViT, EfficientNet, and ResNet, as well as previous state-of-the-art methods, demonstrating its superior performance. p p Our model offers a promising solution for real-world arrhythmia detection and classification, providing: p ul li Enhanced diagnostic efficiency by reducing the workload on cardiologists. li li Personalized treatment options by enabling accurate, continuous monitoring. li li Emergency intervention capabilities through real-time arrhythmia monitoring on wearable devices.This framework has the potential to revolutionize arrhythmia management, improving patient outcomes and advancing the integration of AI in healthcare. Paper published in em Digital Health em : a href https: journals.sagepub.com doi full 10.1177 20552076241278942 Hierarchical Deep Learning for Autonomous Multi-label Arrhythmia Detection and Classification on Real-world Wearable ECG Data a li ul p Additionally, the lack of transparent justifications for decisions made by deep learning models poses challenges for real-world applications. p p In this project, we utilized signals from a portable single-lead ECG device to classify eight arrhythmia classes using convolutional neural networks CNN , achieving 79.91 accuracy on a single heartbeat. To enhance model interpretability, we incorporated Layer-Wise Relevance Propagation LRP , an explainable artificial intelligence XAI algorithm, to analyze the distinctive features of each arrhythmia. p p The XAI analysis revealed that for ventricular premature contraction VPC , the model demonstrated strong activations in the QRS complex and T-wave regions. This aligns with medical interpretations that a wide QRS complex and an oppositely directed T-wave are characteristic of VPC. Such findings emphasize the potential for deep learning models to offer not only accurate predictions but also insights that align with clinical knowledge, supporting biomarker discovery for arrhythmia classification. p p This research contributes to interpretable deep learning for arrhythmia diagnosis by: p ul li Enhancing decision support systems with transparent and interpretable predictions. li li Facilitating biomarker discovery for arrhythmia management. li li Improving trust in AI systems through alignment with clinical knowledge.These advancements pave the way for more explainable, reliable, and effective deep learning models in healthcare. Paper published in em International Conference on Artificial Intelligence in Medicine em AIME 2024: a href https: link.springer.com chapter 10.1007 978-3-031-66535-6 31 Exploring the Possibility of Arrhythmia Interpretation of Time Domain ECG Using XAI: A Preliminary Study a li ul">
        This project addresses the critical challenges of arrhythmia detection and classification, particularly in the context of wearable electrocardiogram (ECG) monitoring devices. Unlike clinically controlled environments, wearable devices operate in noisy, real-world conditions, which complicates the accurate identification of arrhythmias. Additionally, the inherent imbalance in the ratio of normal heartbeats to arrhythmic ones, along with the diverse combinations of arrhythmia types, further compounds the difficulty of the task.

      </p>
    </div>
  </div>
</div>
  </section>


    </main>
    


<footer class="background" style="--image: url('/vova-lab.github.io/preview/pr-/vova-lab.github.io/preview/pr-/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:contact@guangyaoz.com" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0001-8713-9213" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=ETJoidYAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/guangyaoz" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://twitter.com/guangyaoz" data-tooltip="Twitter" data-style="bare" aria-label="Twitter">
      <i class="icon fa-brands fa-twitter"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://youtube.com/guangyaoz" data-tooltip="YouTube" data-style="bare" aria-label="YouTube">
      <i class="icon fa-brands fa-youtube"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2025
    random lab site
      |   Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
